{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9677e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # For Keras layers and comparison\n",
    "\n",
    "# Your custom layers\n",
    "from sequential import Sequential # Adjust path if needed\n",
    "from layers.embedding import Embedding\n",
    "from layers.lstm import LSTM\n",
    "from layers.dense import Dense\n",
    "from layers.bidirectional import Bidirectional\n",
    "# from src.layers.text_vectorization import TextVectorizationWrapper # If testing this too\n",
    "# from src.layers.dropout import Dropout # If testing this too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713db43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100       # Small vocabulary for testing Embedding\n",
    "EMBEDDING_DIM = 8      # Small embedding dimension\n",
    "SEQUENCE_LENGTH = 5    # Short sequences\n",
    "BATCH_SIZE = 2         # Small batch size\n",
    "LSTM_UNITS = 16        # Small number of LSTM units\n",
    "DENSE_UNITS = 3        # For a final Dense layer\n",
    "INPUT_FEATURES_FOR_DENSE = LSTM_UNITS # Or LSTM_UNITS * 2 if from Bidirectional non-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bdb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(keras_out, custom_out, layer_name):\n",
    "    print(f\"\\n--- Comparing: {layer_name} ---\")\n",
    "    print(f\"Keras Output Shape: {keras_out.shape}\")\n",
    "    print(f\"Custom Output Shape: {custom_out.shape}\")\n",
    "    if keras_out.shape != custom_out.shape:\n",
    "        print(\"❌ SHAPES MISMATCH!\")\n",
    "        return False\n",
    "\n",
    "    if np.allclose(keras_out, custom_out, atol=1e-5): # Adjust tolerance\n",
    "        print(\"✅ Outputs MATCH.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Outputs MISMATCH!\")\n",
    "        print(\"   Keras output:\", keras_out.flatten())\n",
    "        print(\"   Custom output:\", custom_out.flatten())\n",
    "        diff = np.abs(keras_out - custom_out)\n",
    "        print(f\"   Max difference: {np.max(diff):.2e} at index {np.unravel_index(np.argmax(diff), diff.shape)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f301d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TESTING EMBEDDING LAYER ===\n",
      "Sample Input Tokens:\n",
      " [[11 69 62 19 19]\n",
      " [88 38 47 99 92]]\n",
      "\n",
      "--- Comparing: Embedding Layer (mask_zero=True) ---\n",
      "Keras Output Shape: (2, 5, 8)\n",
      "Custom Output Shape: (2, 5, 8)\n",
      "❌ Outputs MISMATCH!\n",
      "   Keras output: [ 0.02268467  0.01151795 -0.00092236  0.03119672  0.03414085 -0.01620958\n",
      "  0.0031811   0.04107949 -0.03582083 -0.0316715  -0.00423247  0.01530648\n",
      "  0.04588783  0.03455812 -0.01255903 -0.03345547  0.03453484  0.02269815\n",
      "  0.01000663  0.0467765  -0.01871107  0.01247843  0.04292778  0.04911131\n",
      " -0.03582083 -0.0316715  -0.00423247  0.01530648  0.04588783  0.03455812\n",
      " -0.01255903 -0.03345547  0.01145571  0.00808764  0.02318955  0.0172548\n",
      " -0.04399084 -0.02273153 -0.02674059 -0.01996217 -0.03582083 -0.0316715\n",
      " -0.00423247  0.01530648  0.04588783  0.03455812 -0.01255903 -0.03345547\n",
      "  0.02585098  0.01948514  0.01391545  0.01730097 -0.02558062  0.01476032\n",
      "  0.03769923  0.01493018  0.03031636 -0.00662817  0.03439753  0.01058507\n",
      "  0.04843768 -0.0160344  -0.02365348 -0.03871585 -0.0251271  -0.0466441\n",
      "  0.03183873 -0.00977701  0.04031047 -0.02097138  0.04658082  0.02811039\n",
      "  0.03621825 -0.04663369 -0.01916273  0.00317954  0.01116811  0.02287513\n",
      "  0.00536529  0.00194036]\n",
      "   Custom output: [ 0.02268467  0.01151795 -0.00092236  0.03119672  0.03414085 -0.01620958\n",
      "  0.0031811   0.04107949  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.03453484  0.02269815\n",
      "  0.01000663  0.0467765  -0.01871107  0.01247843  0.04292778  0.04911131\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.01145571  0.00808764  0.02318955  0.0172548\n",
      " -0.04399084 -0.02273153 -0.02674059 -0.01996217  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02585098  0.01948514  0.01391545  0.01730097 -0.02558062  0.01476032\n",
      "  0.03769923  0.01493018  0.03031636 -0.00662817  0.03439753  0.01058507\n",
      "  0.04843768 -0.0160344  -0.02365348 -0.03871585 -0.0251271  -0.0466441\n",
      "  0.03183873 -0.00977701  0.04031047 -0.02097138  0.04658082  0.02811039\n",
      "  0.03621825 -0.04663369 -0.01916273  0.00317954  0.01116811  0.02287513\n",
      "  0.00536529  0.00194036]\n",
      "   Max difference: 4.59e-02 at index (np.int64(0), np.int64(1), np.int64(4))\n",
      "\n",
      "--- Comparing: Embedding Layer ---\n",
      "Keras Output Shape: (2, 5, 8)\n",
      "Custom Output Shape: (2, 5, 8)\n",
      "✅ Outputs MATCH.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n=== TESTING EMBEDDING LAYER ===\")\n",
    "# Sample input: batch of tokenized integer sequences\n",
    "sample_input_tokens = np.random.randint(0, VOCAB_SIZE, size=(BATCH_SIZE, SEQUENCE_LENGTH))\n",
    "print(\"Sample Input Tokens:\\n\", sample_input_tokens)\n",
    "\n",
    "keras_input_emb_mask = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), dtype='int32', name=\"input_emb_mask\")\n",
    "keras_embedding_layer_mask = tf.keras.layers.Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,  # Set mask_zero to True\n",
    "    name=\"keras_embedding_mask_true\"\n",
    ")\n",
    "keras_output_emb_mask = keras_embedding_layer_mask(keras_input_emb_mask)\n",
    "keras_emb_model_mask = tf.keras.Model(inputs=keras_input_emb_mask, outputs=keras_output_emb_mask)\n",
    "\n",
    "# c. Weight Synchronization (Keras -> Custom)\n",
    "# Important: Get weights *after* the layer is built (implicitly by creating the model)\n",
    "keras_embedding_weights_mask = keras_embedding_layer_mask.get_weights()\n",
    "# keras_embedding_weights_mask is a list: [embedding_matrix]\n",
    "# For mask_zero=True, Keras's get_weights() for Embedding will still return the\n",
    "# potentially non-zero weights for the 0-th index. The masking is a computational step.\n",
    "\n",
    "sample_input_tokens_with_zeros = np.random.randint(1, VOCAB_SIZE, size=(BATCH_SIZE, SEQUENCE_LENGTH)) # Start with non-zeros\n",
    "sample_input_tokens_with_zeros[0, 1] = 0  # Introduce a zero in the first sample\n",
    "sample_input_tokens_with_zeros[0, 3] = 0  # Another zero\n",
    "sample_input_tokens_with_zeros[1, 0] = 0\n",
    "\n",
    "# b. Custom Model with mask_zero=True\n",
    "custom_embedding_layer_test_mask = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True  # Set mask_zero to True for your custom layer\n",
    ")\n",
    "# Your load_keras_weights method should handle the mask_zero logic\n",
    "# (i.e., set the 0-th row of its internal weights to zeros)\n",
    "custom_embedding_layer_test_mask.load_keras_weights(keras_embedding_weights_mask)\n",
    "custom_emb_model_mask = Sequential([custom_embedding_layer_test_mask])\n",
    "\n",
    "# d. Forward Pass\n",
    "keras_pred_emb_mask = keras_emb_model_mask.predict(sample_input_tokens_with_zeros, verbose=0)\n",
    "custom_pred_emb_mask = custom_emb_model_mask.predict(sample_input_tokens_with_zeros)\n",
    "\n",
    "# e. Compare\n",
    "compare_outputs(keras_pred_emb_mask, custom_pred_emb_mask, \"Embedding Layer (mask_zero=True)\")\n",
    "\n",
    "# a. Keras Model\n",
    "keras_input_emb = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), dtype='int32', name=\"input_emb\")\n",
    "keras_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=VOCAB_SIZE, \n",
    "    output_dim=EMBEDDING_DIM, \n",
    "    mask_zero=False, # Test both True and False scenarios eventually\n",
    "    name=\"keras_embedding\"\n",
    ")\n",
    "keras_output_emb = keras_embedding_layer(keras_input_emb)\n",
    "keras_emb_model = tf.keras.Model(inputs=keras_input_emb, outputs=keras_output_emb)\n",
    "\n",
    "# c. Weight Synchronization (Keras -> Custom)\n",
    "keras_embedding_weights = keras_embedding_layer.get_weights() \n",
    "# keras_embedding_weights is a list: [embedding_matrix]\n",
    "\n",
    "# b. Custom Model\n",
    "custom_embedding_layer_test = Embedding(\n",
    "    input_dim=VOCAB_SIZE, \n",
    "    output_dim=EMBEDDING_DIM, \n",
    "    mask_zero=False # Match Keras setting\n",
    ")\n",
    "custom_embedding_layer_test.load_keras_weights(keras_embedding_weights) # Load weights from Keras layer\n",
    "custom_emb_model = Sequential([custom_embedding_layer_test])\n",
    "\n",
    "# d. Forward Pass\n",
    "keras_pred_emb = keras_emb_model.predict(sample_input_tokens, verbose=0)\n",
    "custom_pred_emb = custom_emb_model.predict(sample_input_tokens)\n",
    "\n",
    "# e. Compare\n",
    "compare_outputs(keras_pred_emb, custom_pred_emb, \"Embedding Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7dc512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TESTING DENSE LAYER ===\n",
      "\n",
      "--- Comparing: Dense Layer (activation='softmax') ---\n",
      "Keras Output Shape: (2, 3)\n",
      "Custom Output Shape: (2, 3)\n",
      "✅ Outputs MATCH.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n=== TESTING DENSE LAYER ===\")\n",
    "# Sample input: batch of flat vectors (e.g., output of an LSTM with return_sequences=False)\n",
    "sample_input_flat_vectors = np.random.rand(BATCH_SIZE, LSTM_UNITS).astype(np.float32)\n",
    "\n",
    "# a. Keras Model\n",
    "keras_input_dense = tf.keras.layers.Input(shape=(LSTM_UNITS,), name=\"input_dense\")\n",
    "keras_dense_layer = tf.keras.layers.Dense(DENSE_UNITS, activation='softmax', name=\"keras_dense\")\n",
    "keras_output_dense = keras_dense_layer(keras_input_dense)\n",
    "keras_dense_model = tf.keras.Model(inputs=keras_input_dense, outputs=keras_output_dense)\n",
    "\n",
    "# c. Weight Synchronization\n",
    "keras_dense_weights = keras_dense_layer.get_weights()\n",
    "# keras_dense_weights is [kernel, bias]\n",
    "\n",
    "# b. Custom Model\n",
    "custom_dense_layer_test = Dense(units=DENSE_UNITS, activation='softmax')\n",
    "custom_dense_layer_test.load_keras_weights(keras_dense_weights)\n",
    "custom_dense_model = Sequential([custom_dense_layer_test])\n",
    "\n",
    "# d. Forward Pass\n",
    "keras_pred_dense = keras_dense_model.predict(sample_input_flat_vectors, verbose=0)\n",
    "custom_pred_dense = custom_dense_model.predict(sample_input_flat_vectors)\n",
    "\n",
    "# e. Compare\n",
    "compare_outputs(keras_pred_dense, custom_pred_dense, \"Dense Layer (activation='softmax')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89332d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TESTING LSTM LAYER (Unidirectional) ===\n",
      "\n",
      "Testing LSTM with return_sequences=True...\n",
      "\n",
      "--- Comparing: LSTM Layer (return_sequences=True) ---\n",
      "Keras Output Shape: (2, 5, 16)\n",
      "Custom Output Shape: (2, 5, 16)\n",
      "✅ Outputs MATCH.\n",
      "\n",
      "Testing LSTM with return_sequences=False...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218A3558720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "--- Comparing: LSTM Layer (return_sequences=False) ---\n",
      "Keras Output Shape: (2, 16)\n",
      "Custom Output Shape: (2, 16)\n",
      "✅ Outputs MATCH.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n=== TESTING LSTM LAYER (Unidirectional) ===\")\n",
    "# Sample input: batch of sequences of embedding vectors\n",
    "sample_input_vectors = np.random.rand(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM).astype(np.float32)\n",
    "\n",
    "# --- Test with return_sequences=True ---\n",
    "print(\"\\nTesting LSTM with return_sequences=True...\")\n",
    "# a. Keras Model\n",
    "keras_input_lstm_seq = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH, EMBEDDING_DIM), name=\"input_lstm_seq\")\n",
    "keras_lstm_layer_seq = tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True, name=\"keras_lstm_seq\")\n",
    "keras_output_lstm_seq = keras_lstm_layer_seq(keras_input_lstm_seq)\n",
    "keras_lstm_model_seq = tf.keras.Model(inputs=keras_input_lstm_seq, outputs=keras_output_lstm_seq)\n",
    "\n",
    "# c. Weight Synchronization\n",
    "keras_lstm_weights = keras_lstm_layer_seq.get_weights() \n",
    "# keras_lstm_weights is [kernel, recurrent_kernel, bias]\n",
    "\n",
    "# b. Custom Model\n",
    "custom_lstm_layer_test_seq = LSTM(units=LSTM_UNITS, return_sequences=True)\n",
    "custom_lstm_layer_test_seq.load_keras_weights(keras_lstm_weights)\n",
    "custom_lstm_model_seq = Sequential([custom_lstm_layer_test_seq])\n",
    "\n",
    "# d. Forward Pass\n",
    "keras_pred_lstm_seq = keras_lstm_model_seq.predict(sample_input_vectors, verbose=0)\n",
    "custom_pred_lstm_seq = custom_lstm_model_seq.predict(sample_input_vectors)\n",
    "\n",
    "# e. Compare\n",
    "compare_outputs(keras_pred_lstm_seq, custom_pred_lstm_seq, \"LSTM Layer (return_sequences=True)\")\n",
    "\n",
    "# --- Test with return_sequences=False ---\n",
    "print(\"\\nTesting LSTM with return_sequences=False...\")\n",
    "keras_input_lstm_final = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH, EMBEDDING_DIM), name=\"input_lstm_final\")\n",
    "keras_lstm_layer_final = tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=False, name=\"keras_lstm_final\") # Default\n",
    "keras_output_lstm_final = keras_lstm_layer_final(keras_input_lstm_final)\n",
    "keras_lstm_model_final = tf.keras.Model(inputs=keras_input_lstm_final, outputs=keras_output_lstm_final)\n",
    "\n",
    "keras_lstm_weights_final = keras_lstm_layer_final.get_weights()\n",
    "\n",
    "custom_lstm_layer_test_final = LSTM(units=LSTM_UNITS, return_sequences=False)\n",
    "custom_lstm_layer_test_final.load_keras_weights(keras_lstm_weights_final)\n",
    "custom_lstm_model_final = Sequential([custom_lstm_layer_test_final])\n",
    "\n",
    "keras_pred_lstm_final = keras_lstm_model_final.predict(sample_input_vectors, verbose=0)\n",
    "custom_pred_lstm_final = custom_lstm_model_final.predict(sample_input_vectors)\n",
    "compare_outputs(keras_pred_lstm_final, custom_pred_lstm_final, \"LSTM Layer (return_sequences=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798ad873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== TESTING BIDIRECTIONAL(LSTM) LAYER ===\n",
      "\n",
      "Testing Bidirectional(LSTM) with return_sequences=True...\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000218A35C1800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "--- Comparing: Bidirectional(LSTM) (return_sequences=True) ---\n",
      "Keras Output Shape: (2, 5, 32)\n",
      "Custom Output Shape: (2, 5, 32)\n",
      "✅ Outputs MATCH.\n",
      "\n",
      "--- Comparing: Bidirectional's Forward LSTM Component ---\n",
      "Keras Output Shape: (2, 5, 16)\n",
      "Custom Output Shape: (2, 5, 16)\n",
      "✅ Outputs MATCH.\n",
      "\n",
      "--- Comparing: Bidirectional's Backward LSTM Component ---\n",
      "Keras Output Shape: (2, 5, 16)\n",
      "Custom Output Shape: (2, 5, 16)\n",
      "✅ Outputs MATCH.\n",
      "\n",
      "Testing Bidirectional(LSTM) with return_sequences=False...\n",
      "\n",
      "--- Comparing: Bidirectional(LSTM) (return_sequences=False) ---\n",
      "Keras Output Shape: (2, 32)\n",
      "Custom Output Shape: (2, 32)\n",
      "✅ Outputs MATCH.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n\\n=== TESTING BIDIRECTIONAL(LSTM) LAYER ===\")\n",
    "# Sample input: batch of sequences of embedding vectors\n",
    "sample_input_vectors_bidi = np.random.rand(BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_DIM).astype(np.float32)\n",
    "\n",
    "# --- Test with return_sequences=True ---\n",
    "print(\"\\nTesting Bidirectional(LSTM) with return_sequences=True...\")\n",
    "# a. Keras Model\n",
    "keras_input_bidi_seq = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH, EMBEDDING_DIM), name=\"input_bidi_seq\")\n",
    "keras_bidi_layer_seq = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=True), name=\"keras_bidi_seq\"\n",
    ")\n",
    "keras_output_bidi_seq = keras_bidi_layer_seq(keras_input_bidi_seq)\n",
    "keras_bidi_model_seq = tf.keras.Model(inputs=keras_input_bidi_seq, outputs=keras_output_bidi_seq)\n",
    "\n",
    "# c. Weight Synchronization\n",
    "keras_bidi_weights = keras_bidi_layer_seq.get_weights()\n",
    "# keras_bidi_weights is [fwd_kernel, fwd_recurrent_kernel, fwd_bias, bwd_kernel, bwd_recurrent_kernel, bwd_bias]\n",
    "\n",
    "# b. Custom Model\n",
    "# NOTE: Your Bidirectional takes an *instance* of your custom LSTM\n",
    "custom_bidi_lstm_inner_seq = LSTM(units=LSTM_UNITS, return_sequences=True)\n",
    "custom_bidi_layer_test_seq = Bidirectional(layer=custom_bidi_lstm_inner_seq, merge_mode=\"concat\")\n",
    "custom_bidi_layer_test_seq.load_keras_weights(keras_bidi_weights)\n",
    "custom_bidi_model_seq = Sequential([custom_bidi_layer_test_seq])\n",
    "\n",
    "# d. Forward Pass\n",
    "keras_pred_bidi_seq = keras_bidi_model_seq.predict(sample_input_vectors_bidi, verbose=0)\n",
    "custom_pred_bidi_seq = custom_bidi_model_seq.predict(sample_input_vectors_bidi)\n",
    "\n",
    "# e. Compare\n",
    "compare_outputs(keras_pred_bidi_seq, custom_pred_bidi_seq, \"Bidirectional(LSTM) (return_sequences=True)\")\n",
    "\n",
    "# Extract what Keras's forward LSTM part produced (assuming LSTM_UNITS is the units of the inner LSTM)\n",
    "keras_y_forward_component = keras_pred_bidi_seq[:, :, :LSTM_UNITS] # Or :keras_bidi_layer_seq.forward_layer.units\n",
    "custom_y_forward_component = custom_bidi_layer_test_seq.forward_layer.forward(sample_input_vectors_bidi)\n",
    "\n",
    "# Now compare these two:\n",
    "compare_outputs(keras_y_forward_component, custom_y_forward_component, \"Bidirectional's Forward LSTM Component\")\n",
    "\n",
    "LSTM_UNITS_FOR_THIS_TEST = keras_bidi_layer_seq.forward_layer.units # Get units from the Keras layer\n",
    "keras_y_backward_component = keras_pred_bidi_seq[:, :, LSTM_UNITS_FOR_THIS_TEST:]\n",
    "\n",
    "# Custom side - CORRECTED: No manual flipping needed\n",
    "# The corrected LSTM implementation handles the output ordering internally\n",
    "custom_y_backward_component = custom_bidi_layer_test_seq.backward_layer.forward(sample_input_vectors_bidi)\n",
    "\n",
    "# Now compare these two - they should match directly\n",
    "compare_outputs(keras_y_backward_component, custom_y_backward_component, \"Bidirectional's Backward LSTM Component\")\n",
    "\n",
    "# --- Test with return_sequences=False ---\n",
    "# (Similar structure as above, just set return_sequences=False in both Keras LSTM and custom LSTM)\n",
    "print(\"\\nTesting Bidirectional(LSTM) with return_sequences=False...\")\n",
    "keras_input_bidi_final = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH, EMBEDDING_DIM), name=\"input_bidi_final\")\n",
    "keras_bidi_layer_final = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS, return_sequences=False), name=\"keras_bidi_final\"\n",
    ")\n",
    "keras_output_bidi_final = keras_bidi_layer_final(keras_input_bidi_final)\n",
    "keras_bidi_model_final = tf.keras.Model(inputs=keras_input_bidi_final, outputs=keras_output_bidi_final)\n",
    "\n",
    "keras_bidi_weights_final = keras_bidi_layer_final.get_weights()\n",
    "\n",
    "custom_bidi_lstm_inner_final = LSTM(units=LSTM_UNITS, return_sequences=False)\n",
    "custom_bidi_layer_test_final = Bidirectional(layer=custom_bidi_lstm_inner_final, merge_mode=\"concat\")\n",
    "custom_bidi_layer_test_final.load_keras_weights(keras_bidi_weights_final)\n",
    "custom_bidi_model_final = Sequential([custom_bidi_layer_test_final])\n",
    "\n",
    "keras_pred_bidi_final = keras_bidi_model_final.predict(sample_input_vectors_bidi, verbose=0)\n",
    "custom_pred_bidi_final = custom_bidi_model_final.predict(sample_input_vectors_bidi)\n",
    "compare_outputs(keras_pred_bidi_final, custom_pred_bidi_final, \"Bidirectional(LSTM) (return_sequences=False)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
